{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/James/anaconda3/envs/mimic/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check virtual environment: should be: '/Users/James/anaconda3/envs/mimic/bin/python'\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "src_folder = os.path.join(project_root, 'src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import src functions\n",
    "sys.path.insert(0, src_folder)\n",
    "from modeling import *\n",
    "from stats_and_visualisations import *\n",
    "from s3_storage import *\n",
    "from model_access import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---- PARAMETERS\n",
    "iterations_per_model = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Importing done\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "train = from_s3(bucket='mimic-jamesi',\n",
    "               filename='acute_kidney_failure_train.csv',\n",
    "               index_col=0)\n",
    "print('--> Importing done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Cleaning done\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, feature_names = final_cleaning(ids = ['subject_id', 'hadm_id'], target = 'target', train = train)\n",
    "print('--> Cleaning done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary training and validation sets\n",
    "X_train_tmp, X_val_tmp, y_train_tmp, y_val_tmp = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train_logistic(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    clf = LogisticRegression(random_state=0,\n",
    "                             solver='lbfgs',\n",
    "                             class_weight='balanced').fit(X_train, y_train)\n",
    "\n",
    "    train_predict = clf.predict_proba(X_train)\n",
    "    test_predict = clf.predict_proba(X_test)\n",
    "\n",
    "    train_score = roc_auc_score(y_train, train_predict[:,1])\n",
    "    test_score = roc_auc_score(y_test, test_predict[:,1])\n",
    "\n",
    "    print('Logistic Train', train_score)\n",
    "    print('Logistic Test', test_score)\n",
    "    \n",
    "    return train_score, test_score, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Train 0.835335549500201\n",
      "Logistic Test 0.843580660375758\n"
     ]
    }
   ],
   "source": [
    "train_score, test_score, logistic_model = train_logistic(X_train_tmp, X_val_tmp, y_train_tmp, y_val_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as Pickle\n",
    "save_pickle(logistic_model, 'logistic_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_search(model, random_grid, scoring, cv, n_iter, X_train, y_train):\n",
    "    \n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    m = model\n",
    "    print('--> Model defined')\n",
    "\n",
    "    random_search_model = RandomizedSearchCV(estimator = m, scoring=scoring,\n",
    "                                   param_distributions = random_grid,\n",
    "                                   n_iter = n_iter, cv = cv, verbose=0,\n",
    "                                   random_state=8, n_jobs = -1,\n",
    "                                   return_train_score=True)\n",
    "    print('--> Random search defined')\n",
    "\n",
    "    # Fit the random search model\n",
    "    random_search_model.fit(X_train, y_train)\n",
    "    print('--> Fitting done')\n",
    "\n",
    "    # Print the best CV score\n",
    "    print('--> Best CV Score: ', random_search_model.best_score_)\n",
    "    \n",
    "    return random_search_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_results(random_grid, random_search_model, training_score, cv_score, fit_time):\n",
    "    \n",
    "    # Clean the CV results df, keeping only columns that are needed\n",
    "    keep_cols = ['param_{}'.format(param) for param in random_grid.keys()]\n",
    "    keep_cols = keep_cols + [training_score, cv_score, fit_time]\n",
    "    results = pd.DataFrame(random_search_model.cv_results_)[keep_cols]\n",
    "\n",
    "    # Visualise best CV score by run\n",
    "    best_cv_by_run(results, cv_score)\n",
    "\n",
    "    # Visualise the scores by single hyperparameters\n",
    "    plot_single_results(results, training_score, cv_score, fit_time)\n",
    "\n",
    "    # Visualise the scores by double hyperparameters\n",
    "    plot_double_results(results, [training_score, cv_score, fit_time])\n",
    "\n",
    "    results.sort_values(by=cv_score, ascending=False, inplace=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "criterion = ['gini', 'entropy']\n",
    "class_weight = ['balanced']\n",
    "max_features = list(np.arange(2, X_train.shape[1]))\n",
    "max_depth = list(np.arange(1, 100))\n",
    "max_depth.append(None)\n",
    "min_samples_split = list(np.arange(2, 250))\n",
    "min_samples_leaf = list(np.arange(1, 250))\n",
    "\n",
    "# Create the random grid\n",
    "dt_random_grid = {'criterion': criterion,\n",
    "                  'class_weight': class_weight,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "print('--> Grid defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the random search model\n",
    "dt_random_search_model = run_random_search(model=DecisionTreeClassifier(), random_grid=dt_random_grid,\n",
    "                                        scoring='roc_auc', cv=5, n_iter=iterations_per_model, \n",
    "                                        X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the results to find optimal hyperparameters\n",
    "dt_results = visualise_results(random_grid=dt_random_grid, random_search_model=dt_random_search_model,\n",
    "                            training_score='mean_train_score', cv_score='mean_test_score', fit_time='mean_fit_time')\n",
    "dt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "n_estimators = list(np.arange(20, 3000, 5))\n",
    "max_features = list(np.arange(2, X_train.shape[1]))\n",
    "max_depth = list(np.arange(1, 100))\n",
    "max_depth.append(None)\n",
    "min_samples_split = list(np.arange(2, 250))\n",
    "min_samples_leaf = list(np.arange(1, 250))\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "rf_random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "print('--> Grid defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the random search model\n",
    "rf_random_search_model = run_random_search(model=RandomForestRegressor(), random_grid=rf_random_grid,\n",
    "                                        scoring='roc_auc', cv=5, n_iter=iterations_per_model, \n",
    "                                        X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the results to find optimal hyperparameters\n",
    "rf_results = visualise_results(random_grid=rf_random_grid, random_search_model=rf_random_search_model,\n",
    "                            training_score='mean_train_score', cv_score='mean_test_score', fit_time='mean_fit_time')\n",
    "rf_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mimic]",
   "language": "python",
   "name": "conda-env-mimic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
