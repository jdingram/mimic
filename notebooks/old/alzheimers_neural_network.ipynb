{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import statistics\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import talos as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/James/anaconda3/envs/mimic/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check virtual environment: should be: '/Users/James/anaconda3/envs/mimic/bin/python'\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "src_folder = os.path.join(project_root, 'src')\n",
    "\n",
    "src_preparation_folder = os.path.join(src_folder, 'preparation')\n",
    "src_processing_folder = os.path.join(src_folder, 'processing')\n",
    "src_modeling_folder = os.path.join(src_folder, 'modeling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "# Import src functions\n",
    "sys.path.insert(0, src_preparation_folder)\n",
    "from import_data import get_table\n",
    "from import_data import get_patient_admissions_diagnoses\n",
    "from import_data import get_admission_data\n",
    "from import_data import get_chartevents\n",
    "from import_data import get_labevents\n",
    "from extract_codes import find_ndc_codes\n",
    "\n",
    "sys.path.insert(0, src_processing_folder)\n",
    "from stats import plot_KDE\n",
    "from stats import plot_perc_bar_chart\n",
    "from stats import compare_groups\n",
    "\n",
    "sys.path.insert(0, src_modeling_folder)\n",
    "from models import train_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM ARRAYS FOR TESTING AND BUILDING MODELS\n",
    "#features = np.random.rand(100,100)\n",
    "#labels = np.random.randint(0,2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(os.path.join(os.getcwd(), os.pardir, 'data', 'alzheimers_features.npy'))\n",
    "labels = np.load(os.path.join(os.getcwd(), os.pardir, 'data', 'alzheimers_labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4380, 1526)\n",
      "(4380,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_keras_nn(features, labels, test_size):\n",
    "    \n",
    "    # Split into test and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size = test_size)\n",
    "    print(len(X_train))\n",
    "    print(len(X_val))\n",
    "        \n",
    "    # --- NEURAL NETWORK\n",
    "\n",
    "    #Initializing Neural Network\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Adding the input layer and the first hidden layer\n",
    "    classifier.add(Dense(output_dim = 250, init = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n",
    "    # Adding the second hidden layer\n",
    "    classifier.add(Dense(output_dim = 250, init = 'uniform', activation = 'relu'))\n",
    "    # Adding the third hidden layer\n",
    "    classifier.add(Dense(output_dim = 250, init = 'uniform', activation = 'relu'))\n",
    "    # Adding the fourth hidden layer\n",
    "    classifier.add(Dense(output_dim = 250, init = 'uniform', activation = 'relu'))\n",
    "    # Adding the fifth hidden layer\n",
    "    classifier.add(Dense(output_dim = 250, init = 'uniform', activation = 'relu'))\n",
    "    # Adding the sixth hidden layer\n",
    "    classifier.add(Dense(output_dim = 250, init = 'uniform', activation = 'relu'))\n",
    "    # Adding the seventh hidden layer\n",
    "    classifier.add(Dense(output_dim = 250, init = 'uniform', activation = 'relu'))\n",
    "    # Adding the eighth hidden layer\n",
    "    classifier.add(Dense(output_dim = 250, init = 'uniform', activation = 'relu'))\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "    \n",
    "    #Optimiser\n",
    "    sgd = keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False)\n",
    "\n",
    "    # Compiling Neural Network\n",
    "    classifier.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    # Fitting our model \n",
    "    classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 10)\n",
    "    \n",
    "    # Find train and test ROC scores\n",
    "    y_predict = classifier.predict(X_val)\n",
    "    val_ROC_sc = roc_auc_score(y_val, y_predict)\n",
    "    \n",
    "    y_predict = classifier.predict(X_train)\n",
    "    train_ROC_sc = roc_auc_score(y_train, y_predict)\n",
    "    \n",
    "    print('Train is ' + str(train_ROC_sc))\n",
    "    print('Val is ' + str(val_ROC_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3504\n",
      "876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=1526, units=250, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=250, kernel_initializer=\"uniform\")`\n",
      "  app.launch_new_instance()\n",
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=250, kernel_initializer=\"uniform\")`\n",
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=250, kernel_initializer=\"uniform\")`\n",
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=250, kernel_initializer=\"uniform\")`\n",
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=250, kernel_initializer=\"uniform\")`\n",
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=250, kernel_initializer=\"uniform\")`\n",
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=250, kernel_initializer=\"uniform\")`\n",
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3504/3504 [==============================] - 3s 976us/step - loss: 0.2846 - acc: 0.9289\n",
      "Epoch 2/10\n",
      "3504/3504 [==============================] - 2s 671us/step - loss: 0.2527 - acc: 0.9307\n",
      "Epoch 3/10\n",
      "3504/3504 [==============================] - 2s 686us/step - loss: 0.2525 - acc: 0.9307\n",
      "Epoch 4/10\n",
      "3504/3504 [==============================] - 2s 671us/step - loss: 0.2526 - acc: 0.9307\n",
      "Epoch 5/10\n",
      "3504/3504 [==============================] - 2s 705us/step - loss: 0.2527 - acc: 0.9307\n",
      "Epoch 6/10\n",
      "3504/3504 [==============================] - 3s 741us/step - loss: 0.2525 - acc: 0.9307\n",
      "Epoch 7/10\n",
      "3504/3504 [==============================] - 3s 742us/step - loss: 0.2526 - acc: 0.9307\n",
      "Epoch 8/10\n",
      "3504/3504 [==============================] - 3s 775us/step - loss: 0.2527 - acc: 0.9307\n",
      "Epoch 9/10\n",
      "3504/3504 [==============================] - 3s 792us/step - loss: 0.2523 - acc: 0.9307\n",
      "Epoch 10/10\n",
      "3504/3504 [==============================] - 3s 775us/step - loss: 0.2523 - acc: 0.9307\n",
      "Train is 0.6998787263872956\n",
      "Val is 0.6816083903525313\n"
     ]
    }
   ],
   "source": [
    "train_keras_nn(features=features, labels=labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have to make sure to input data and params into the function\n",
    "def train_keras_nn_v2(x_train, y_train, x_val, y_val, params):\n",
    "\n",
    "    # next we can build the model exactly like we would normally do it\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=x_train.shape[1],\n",
    "                    activation=params['activation'],\n",
    "                    kernel_initializer='normal'))\n",
    "    \n",
    "    #model.add(dropout=(params['dropout']))\n",
    "    \n",
    "    # if we want to also test for number of layers and shapes, that's possible\n",
    "    #hidden_layers(model, params, 1)\n",
    "   \n",
    "    # then we finish again with completely standard Keras way\n",
    "    model.add(Dense(1, activation=params['last_activation'],\n",
    "                    kernel_initializer='normal'))\n",
    "    \n",
    "    model.compile(loss=params['losses'],\n",
    "                  # here we add a regulizer normalization function from Talos\n",
    "                  optimizer=params['optimizer'],\n",
    "                  lr=params['lr'],\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0)\n",
    "    \n",
    "    # finally we have to make sure that history object and model are returned\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can go ahead and set the parameter space\n",
    "p = {'lr': (0.5, 5, 10),\n",
    "     'first_neuron':[4, 8, 16, 32, 64],\n",
    "     'hidden_layers':[0, 1, 2],\n",
    "     'batch_size': (2, 30, 10),\n",
    "     'epochs': [10],\n",
    "     'dropout': (0, 0.5, 5),\n",
    "     'weight_regulizer':[None],\n",
    "     'emb_output_dims': [None],\n",
    "     'shape':['brick','long_funnel'],\n",
    "     'optimizer': ['Adam', 'Nadam', 'RMSprop'],\n",
    "     'losses': ['logcosh', 'binary_crossentropy'],\n",
    "     'activation':['relu', 'elu'],\n",
    "     'last_activation': ['sigmoid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1800 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('Some keys in session_kwargs are not supported at this time: %s', dict_keys(['lr']))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-4e8b6e41e507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first_test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             experiment_no='1')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mimic/lib/python3.6/site-packages/talos/scan/Scan.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, params, model, dataset_name, experiment_no, x_val, y_val, val_split, shuffle, round_limit, grid_downsample, random_method, seed, search_method, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, reduce_loss, last_epoch_value, talos_log_name, clear_tf_session, functional_model, disable_progress_bar, print_params, debug)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# input parameters section ends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_null\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mimic/lib/python3.6/site-packages/talos/scan/Scan.py\u001b[0m in \u001b[0;36mruntime\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mimic/lib/python3.6/site-packages/talos/scan/scan_run.py\u001b[0m in \u001b[0;36mscan_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m                      disable=self.disable_progress_bar)\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_log\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mimic/lib/python3.6/site-packages/talos/scan/scan_round.py\u001b[0m in \u001b[0;36mscan_round\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0m_hr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"unsupported operand type(s) for +: 'int' and 'numpy.str_'\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mimic/lib/python3.6/site-packages/talos/model/ingest_model.py\u001b[0m in \u001b[0;36mingest_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                       self.round_params)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-86e04ec42b05>\u001b[0m in \u001b[0;36mtrain_keras_nn_v2\u001b[0;34m(x_train, y_train, x_val, y_val, params)\u001b[0m\n\u001b[1;32m     27\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                         verbose=0)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# finally we have to make sure that history object and model are returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mimic/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mimic/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    517\u001b[0m                     \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                     **self._function_kwargs)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mimic/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[1;32m   2742\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Invalid argument \"%s\" passed to K.function with TensorFlow backend'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2743\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2744\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mimic/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name, **session_kwargs)\u001b[0m\n\u001b[1;32m   2573\u001b[0m             raise ValueError('Some keys in session_kwargs are not '\n\u001b[1;32m   2574\u001b[0m                              \u001b[0;34m'supported at this '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2575\u001b[0;31m                              'time: %s', session_kwargs.keys())\n\u001b[0m\u001b[1;32m   2576\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2577\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Some keys in session_kwargs are not supported at this time: %s', dict_keys(['lr']))"
     ]
    }
   ],
   "source": [
    "# and run the experiment\n",
    "t = ta.Scan(x=features,\n",
    "            y=labels,\n",
    "            model=train_keras_nn_v2,\n",
    "            grid_downsample=0.01, \n",
    "            params=p,\n",
    "            dataset_name='first_test',\n",
    "            experiment_no='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mimic]",
   "language": "python",
   "name": "conda-env-mimic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
