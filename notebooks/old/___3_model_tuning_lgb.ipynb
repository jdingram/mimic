{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import statistics\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some display options\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/James/anaconda3/envs/mimic/bin/python'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check virtual environment: should be: '/Users/James/anaconda3/envs/mimic/bin/python'\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "src_folder = os.path.join(project_root, 'src')\n",
    "\n",
    "src_preparation_folder = os.path.join(src_folder, 'preparation')\n",
    "src_processing_folder = os.path.join(src_folder, 'processing')\n",
    "src_modeling_folder = os.path.join(src_folder, 'modeling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import src functions\n",
    "# sys.path.insert(0, src_preparation_folder)\n",
    "# from import_data import get_table\n",
    "# from import_data import get_data_simple\n",
    "# from import_data import get_patient_admissions_diagnoses\n",
    "# from import_data import get_admission_data\n",
    "# from import_data import get_chartevents\n",
    "# from import_data import get_labevents\n",
    "# from extract_codes import find_ndc_codes\n",
    "\n",
    "# sys.path.insert(0, src_processing_folder)\n",
    "# from stats import plot_KDE\n",
    "# from stats import plot_perc_bar_chart\n",
    "# from stats import compare_groups\n",
    "# from stats import graph_comparisons\n",
    "# from patient_selection import select_test_groups\n",
    "# from clean import replace_itemid_with_label\n",
    "# from clean import find_populated_cols\n",
    "\n",
    "# sys.path.insert(0, src_modeling_folder)\n",
    "# from models import train_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train = pd.read_csv(os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'data', 'acute_respiratory_failure_train.csv')),index_col=0)\n",
    "test = pd.read_csv(os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'data', 'acute_respiratory_failure_test.csv')),index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cleaning(train, test):\n",
    "\n",
    "    # Shuffle\n",
    "    train = train.sample(frac=1).reset_index(drop=True)\n",
    "    test = test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Split features and labels\n",
    "    X_train = train.drop(columns=['subject_id', 'hadm_id', 'target'])\n",
    "    y_train = np.array(train.target.tolist())\n",
    "    \n",
    "    X_test = test.drop(columns=['subject_id', 'hadm_id', 'target'])\n",
    "    y_test = np.array(test.target.tolist())\n",
    "\n",
    "    # Impute missing values\n",
    "    imputer = Imputer(strategy = 'median')\n",
    "    imputer.fit(X_train)\n",
    "    X_train = imputer.transform(X_train)\n",
    "    X_test = imputer.transform(X_test)\n",
    "\n",
    "    # Scale each feature to 0-1\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1)) \n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22732, 42)\n",
      "(5684, 42)\n",
      "(22732,)\n",
      "(5684,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = final_cleaning(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb(X_train, y_train, n_folds, params, eval_metric, early_stopping_rounds):\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(X_train.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    print('LGB starting')\n",
    "        \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(X_train):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features  = X_train[train_indices]\n",
    "        train_labels = [x for i,x in enumerate(y_train) if i in train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features = X_train[valid_indices]\n",
    "        valid_labels = [x for i,x in enumerate(y_train) if i in valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = eval_metric,\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], early_stopping_rounds = early_stopping_rounds, verbose=500)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid'][eval_metric]\n",
    "        train_score = model.best_score_['train'][eval_metric]\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(y_train, out_of_fold)\n",
    "\n",
    "    # Overall training score\n",
    "    train_auc = np.mean(train_scores)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(train_auc)\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores})\n",
    "    \n",
    "    print(metrics)\n",
    "    \n",
    "    return metrics, train_auc, valid_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "    'num_leaves': list(range(10, 150)),\n",
    "    'learning_rate': list(np.linspace(0.001, 0.5)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_data_in_leaf': list(range(10, 250, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.001, 1)),\n",
    "    'subsample': list(np.linspace(0.5, 1)),\n",
    "    'is_unbalance': [True, False],\n",
    "    'min_split_gain': list(np.linspace(0.001, 1)),\n",
    "    'min_data_in_leaf': list(np.arange(1, 200, 3)),\n",
    "    'n_estimators': list(np.arange(100, 20100, 1000))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lgb(X_train, y_train, param_grid, runs):\n",
    "    \n",
    "    ## -- Create output dataframe showing scores and associated hyperparameters\n",
    "    df_cols = list(param_grid.keys())\n",
    "    df_cols = df_cols + ['training_score', 'valid_score']\n",
    "\n",
    "    runs_df = pd.DataFrame(columns=df_cols)\n",
    "    total_runs = runs\n",
    "    run =0\n",
    "\n",
    "    while run < total_runs:\n",
    "\n",
    "        run += 1\n",
    "\n",
    "        # Select the random parameters\n",
    "        random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "\n",
    "        print('=========')\n",
    "        print('RUN IS ' + str(run))\n",
    "        print('=========')\n",
    "\n",
    "        metrics, train_score, valid_score= train_lgb(X_train=X_train,\n",
    "                                                      y_train=y_train,\n",
    "                                                      n_folds = 4,\n",
    "                                                      params = random_params,\n",
    "                                                      eval_metric = 'auc',\n",
    "                                                      early_stopping_rounds = 250)\n",
    "\n",
    "        temp_df = pd.DataFrame(columns=df_cols)\n",
    "\n",
    "        for c in list(param_grid.keys()):\n",
    "            temp_df.loc[0, c] = random_params[c]\n",
    "\n",
    "        temp_df.loc[0, 'training_score'] = train_score\n",
    "        temp_df.loc[0, 'valid_score'] = valid_score\n",
    "\n",
    "        runs_df = runs_df.append(temp_df)\n",
    "\n",
    "        del temp_df, train_score, valid_score\n",
    "        \n",
    "    return runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========\n",
      "RUN IS 1\n",
      "=========\n",
      "LGB starting\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's auc: 0.794721\tvalid's binary_logloss: 0.47985\ttrain's auc: 0.93719\ttrain's binary_logloss: 0.366081\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.793102\tvalid's binary_logloss: 0.463278\ttrain's auc: 0.96019\ttrain's binary_logloss: 0.316659\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.784947\tvalid's binary_logloss: 0.473502\ttrain's auc: 0.962549\ttrain's binary_logloss: 0.309117\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's auc: 0.798153\tvalid's binary_logloss: 0.478801\ttrain's auc: 0.931497\ttrain's binary_logloss: 0.376605\n",
      "      fold     train     valid\n",
      "0        0  0.937190  0.794721\n",
      "1        1  0.960190  0.793102\n",
      "2        2  0.962549  0.784947\n",
      "3        3  0.931497  0.798153\n",
      "4  overall  0.947857  0.791983\n",
      "=========\n",
      "RUN IS 2\n",
      "=========\n",
      "LGB starting\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid's auc: 0.804286\tvalid's binary_logloss: 0.36124\ttrain's auc: 0.958428\ttrain's binary_logloss: 0.225362\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's auc: 0.806937\tvalid's binary_logloss: 0.348908\ttrain's auc: 0.929723\ttrain's binary_logloss: 0.264763\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's auc: 0.796306\tvalid's binary_logloss: 0.371575\ttrain's auc: 0.920296\ttrain's binary_logloss: 0.272097\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.806726\tvalid's binary_logloss: 0.359093\ttrain's auc: 0.890091\ttrain's binary_logloss: 0.312432\n",
      "      fold     train     valid\n",
      "0        0  0.958428  0.804286\n",
      "1        1  0.929723  0.806937\n",
      "2        2  0.920296  0.796306\n",
      "3        3  0.890091  0.806726\n",
      "4  overall  0.924634  0.801788\n",
      "=========\n",
      "RUN IS 3\n",
      "=========\n",
      "LGB starting\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.769756\tvalid's binary_logloss: 0.38085\ttrain's auc: 0.900512\ttrain's binary_logloss: 0.291183\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's auc: 0.771493\tvalid's binary_logloss: 0.370601\ttrain's auc: 0.919165\ttrain's binary_logloss: 0.273253\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.763262\tvalid's binary_logloss: 0.391537\ttrain's auc: 0.905713\ttrain's binary_logloss: 0.285988\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[208]\tvalid's auc: 0.791523\tvalid's binary_logloss: 0.384668\ttrain's auc: 0.999516\ttrain's binary_logloss: 0.0883983\n",
      "      fold     train     valid\n",
      "0        0  0.900512  0.769756\n",
      "1        1  0.919165  0.771493\n",
      "2        2  0.905713  0.763262\n",
      "3        3  0.999516  0.791523\n",
      "4  overall  0.931227  0.648019\n",
      "=========\n",
      "RUN IS 4\n",
      "=========\n",
      "LGB starting\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.789487\tvalid's binary_logloss: 0.371864\ttrain's auc: 0.856059\ttrain's binary_logloss: 0.3196\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.788724\tvalid's binary_logloss: 0.359226\ttrain's auc: 0.844788\ttrain's binary_logloss: 0.33235\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.788711\tvalid's binary_logloss: 0.378182\ttrain's auc: 0.858923\ttrain's binary_logloss: 0.315303\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid's auc: 0.787393\tvalid's binary_logloss: 0.367326\ttrain's auc: 0.849689\ttrain's binary_logloss: 0.327037\n",
      "      fold     train     valid\n",
      "0        0  0.856059  0.789487\n",
      "1        1  0.844788  0.788724\n",
      "2        2  0.858923  0.788711\n",
      "3        3  0.849689  0.787393\n",
      "4  overall  0.852365  0.788302\n",
      "=========\n",
      "RUN IS 5\n",
      "=========\n",
      "LGB starting\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "[500]\tvalid's auc: 0.806732\tvalid's binary_logloss: 0.427791\ttrain's auc: 0.858352\ttrain's binary_logloss: 0.416726\n",
      "Early stopping, best iteration is:\n",
      "[303]\tvalid's auc: 0.804295\tvalid's binary_logloss: 0.42432\ttrain's auc: 0.852223\ttrain's binary_logloss: 0.415862\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "[500]\tvalid's auc: 0.803639\tvalid's binary_logloss: 0.423258\ttrain's auc: 0.858144\ttrain's binary_logloss: 0.419888\n",
      "Early stopping, best iteration is:\n",
      "[262]\tvalid's auc: 0.799823\tvalid's binary_logloss: 0.417214\ttrain's auc: 0.850593\ttrain's binary_logloss: 0.420381\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid's auc: 0.794648\tvalid's binary_logloss: 0.441854\ttrain's auc: 0.846354\ttrain's binary_logloss: 0.429087\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "[500]\tvalid's auc: 0.806089\tvalid's binary_logloss: 0.426222\ttrain's auc: 0.859085\ttrain's binary_logloss: 0.418494\n",
      "Early stopping, best iteration is:\n",
      "[282]\tvalid's auc: 0.803218\tvalid's binary_logloss: 0.422219\ttrain's auc: 0.851936\ttrain's binary_logloss: 0.417917\n",
      "      fold     train     valid\n",
      "0        0  0.852223  0.804295\n",
      "1        1  0.850593  0.799823\n",
      "2        2  0.846354  0.794648\n",
      "3        3  0.851936  0.803218\n",
      "4  overall  0.850276  0.723288\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boosting_type</th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>subsample_for_bin</th>\n",
       "      <th>min_data_in_leaf</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>is_unbalance</th>\n",
       "      <th>min_split_gain</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>training_score</th>\n",
       "      <th>valid_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gbdt</td>\n",
       "      <td>77</td>\n",
       "      <td>0.245408</td>\n",
       "      <td>140000</td>\n",
       "      <td>118</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.877673</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>False</td>\n",
       "      <td>0.775735</td>\n",
       "      <td>9100</td>\n",
       "      <td>0.924634</td>\n",
       "      <td>0.801788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gbdt</td>\n",
       "      <td>56</td>\n",
       "      <td>0.428714</td>\n",
       "      <td>40000</td>\n",
       "      <td>88</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.0816327</td>\n",
       "      <td>0.408755</td>\n",
       "      <td>0.581633</td>\n",
       "      <td>True</td>\n",
       "      <td>0.225265</td>\n",
       "      <td>18100</td>\n",
       "      <td>0.947857</td>\n",
       "      <td>0.791983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goss</td>\n",
       "      <td>117</td>\n",
       "      <td>0.398163</td>\n",
       "      <td>160000</td>\n",
       "      <td>145</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.0612245</td>\n",
       "      <td>0.571857</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>False</td>\n",
       "      <td>0.531082</td>\n",
       "      <td>14100</td>\n",
       "      <td>0.852365</td>\n",
       "      <td>0.788302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gbdt</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.36798</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>True</td>\n",
       "      <td>0.81651</td>\n",
       "      <td>3100</td>\n",
       "      <td>0.850276</td>\n",
       "      <td>0.723288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dart</td>\n",
       "      <td>107</td>\n",
       "      <td>0.5</td>\n",
       "      <td>120000</td>\n",
       "      <td>61</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.18449</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266041</td>\n",
       "      <td>18100</td>\n",
       "      <td>0.931227</td>\n",
       "      <td>0.648019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  boosting_type num_leaves learning_rate subsample_for_bin min_data_in_leaf  \\\n",
       "0          gbdt         77      0.245408            140000              118   \n",
       "0          gbdt         56      0.428714             40000               88   \n",
       "0          goss        117      0.398163            160000              145   \n",
       "0          gbdt         40         0.001             20000               25   \n",
       "0          dart        107           0.5            120000               61   \n",
       "\n",
       "  reg_alpha reg_lambda colsample_bytree subsample is_unbalance min_split_gain  \\\n",
       "0  0.326531   0.346939         0.877673  0.918367        False       0.775735   \n",
       "0  0.755102  0.0816327         0.408755  0.581633         True       0.225265   \n",
       "0  0.489796  0.0612245         0.571857  0.693878        False       0.531082   \n",
       "0  0.530612   0.469388          0.36798  0.765306         True        0.81651   \n",
       "0  0.285714   0.918367          0.18449  0.959184        False       0.266041   \n",
       "\n",
       "  n_estimators training_score valid_score  \n",
       "0         9100       0.924634    0.801788  \n",
       "0        18100       0.947857    0.791983  \n",
       "0        14100       0.852365    0.788302  \n",
       "0         3100       0.850276    0.723288  \n",
       "0        18100       0.931227    0.648019  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs_df = tune_lgb(X_train, y_train, param_grid, runs=5)\n",
    "runs_df.sort_values(by='valid_score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mimic]",
   "language": "python",
   "name": "conda-env-mimic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
