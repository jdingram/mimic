{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate datasets\n",
    "\n",
    "The purpose of this notebook is to create two foundational datasets:<br/>\n",
    "1) admission_diagnosis_table: This dataset contains the most important patient demographic and diagnosis data<br/>\n",
    "for all admissions.<br/>\n",
    "2) last_reading: This dataset contains the final reading for 35 chart and lab items for each admission.<br/><br/>\n",
    "The intention for creating these datasets is that they provide all the data required for selecting the patient<br/>\n",
    "groups and building the model without the need to directly query the raw data again.<br/><br/>\n",
    "Both datasets are then saved on AWS S3 to be accessed in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MUST DELETE\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "#pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "# Set up paths & import functions\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "src_folder = os.path.join(project_root, 'src')\n",
    "sys.path.insert(0, src_folder)\n",
    "from generate_datasets import *\n",
    "from stats_and_visualisations import *\n",
    "from s3_storage import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## admission_diagnosis_table\n",
    "This dataset includes the most important patient demographic and diagnosis data for all admissions.<br/><br/>\n",
    "It draws from 3 primary tables in the raw data:<br/>\n",
    "1) patients - for data specific to patients such as their gender and year of birth.<br/>\n",
    "2) admissions - to find all admissions associated with each patient.<br/>\n",
    "3) diagnoses_icd - to find all diagnoses made during each admission.<br/><br/>\n",
    "The final dataset is at the diagnosis level, meaning there is 1 row per diagnosis per admission.<br/><br/>\n",
    "Several additional columns are added to the dataset to enable easier analysis:<br/>\n",
    "1) total_admissions: gives the total number of admissions associated with each subject_id.<br/>\n",
    "2) admission_number: gives a chronological representation of each admission for each patient, eg 1 would be a<br/>\n",
    "patient's 1st admission, 2 would be a patient's 2nd admission, etc.<br/>\n",
    "3) age_on_admission: the difference between dob and admittime. This is correct for all patients under 89, however<br/>\n",
    "the dob was shifted in the raw data for all patients over 89 to obscure their age to comply with HIIPA.<br/>\n",
    "Therefore, all patients over 89 are listed as being 89, and an additional flag age_on_admission_shifted was<br/>\n",
    "created to show when this is the case.<br/>\n",
    "4) age_adm_bucket: the above age data bucketed into the ranges <45, 45-60, 60-75, 75-89, 89.<br/>\n",
    "5) ethnicity_simple: cleaned version of the standard ethnicity data to reduce and simplify the ethnicity buckets.<br/> For detailed analysis of ethnicity this shouldn't be used.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "could not connect to server: Connection refused\n\tIs the server running on host \"localhost\" (127.0.0.1) and accepting\n\tTCP/IP connections on port 5432?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5f1d479497d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the admission_diagnosis_table and save to S3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madmission_diagnosis_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_admission_diagnosis_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mto_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madmission_diagnosis_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mimic-jamesi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'admission_diagnosis_table.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0madmission_diagnosis_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mimic/src/generate_datasets.py\u001b[0m in \u001b[0;36mcreate_admission_diagnosis_table\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m                    \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0mSELECT\u001b[0m \u001b[0mDISTINCT\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                             \u001b[0msubject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpire_flag\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                             FROM mimiciii.patients\")\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Get admission data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mimic/src/generate_datasets.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;31m# Select DB location and execute the specified query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"host=localhost dbname=mimic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mdsn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dsn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwasync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcursor_factory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: could not connect to server: Connection refused\n\tIs the server running on host \"localhost\" (127.0.0.1) and accepting\n\tTCP/IP connections on port 5432?\n"
     ]
    }
   ],
   "source": [
    "# Create the admission_diagnosis_table and save to S3\n",
    "admission_diagnosis_table = create_admission_diagnosis_table()\n",
    "to_s3(df=admission_diagnosis_table, bucket='mimic-jamesi', filename='admission_diagnosis_table.csv')\n",
    "admission_diagnosis_table.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first_reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import chart IDs\n",
    "item_lookup = from_s3(bucket='mimic-jamesi', filename='item_lookup.csv', index_col=0)\n",
    "item_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all readings for these IDs from the raw data\n",
    "ids = tuple(item_lookup.itemid.tolist())\n",
    "\n",
    "# Lab events\n",
    "lab = get_data(query = \"SELECT DISTINCT\\\n",
    "                            subject_id\\\n",
    "                            ,hadm_id\\\n",
    "                            ,charttime\\\n",
    "                            ,itemid\\\n",
    "                            ,valuenum\\\n",
    "                        FROM\\\n",
    "                            mimiciii.labevents\\\n",
    "                        WHERE valuenum IS NOT null\\\n",
    "                        AND itemid IN {}\".format(ids))\n",
    "\n",
    "# Lab events\n",
    "chart = get_data(query = \"SELECT DISTINCT\\\n",
    "                            subject_id\\\n",
    "                            ,hadm_id\\\n",
    "                            ,charttime\\\n",
    "                            ,itemid\\\n",
    "                            ,valuenum\\\n",
    "                        FROM\\\n",
    "                            mimiciii.chartevents\\\n",
    "                        WHERE valuenum IS NOT null\\\n",
    "                        AND itemid IN {}\".format(ids))\n",
    "\n",
    "df = lab.append(chart)\n",
    "\n",
    "# Merge on lookup so that identical concepts can be combined, identified through new_id\n",
    "df = pd.merge(df, item_lookup[['itemid', 'new_id', 'name']], how='left', left_on='itemid', right_on='itemid')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_itemids(df):\n",
    "\n",
    "    '''\n",
    "    This function takes a dataframe containing chart & lab events and outputs visualisations and stats for\n",
    "    all itemids that are contained. The purpose of this is that if there are multiple itemids that seem to contain\n",
    "    a similar concept, their values can be compared to see whether this is the case.\n",
    "    \n",
    "    The input dataframe must contain the following columns:\n",
    "    1) itemid: used to identify the chart/ lab event items\n",
    "    2) valuenum: contains the numerical values of the observations for each itemid\n",
    "    3) hadm_id: used to identify each admission\n",
    "    \n",
    "    The dataframe can be at either the  admission or chart observation level, but the output will reflect this.\n",
    "    i.e, if the input is at the admission level then the output stats will be at the admission level, whereas if\n",
    "    the input is at the chart observation level then the output stats will be for every observation recorded across\n",
    "    all admissions\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df.drop_duplicates(inplace=True)    \n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Find all itemids so that they can be compared against each other\n",
    "    item_ids = df.itemid.unique().tolist()\n",
    "\n",
    "    # --- Plot a KDE: 1 line for each itemid\n",
    "    plt.figure(figsize = (7, 5))\n",
    "    for i in item_ids:\n",
    "        sns.kdeplot(df.loc[df['itemid'] == i, 'valuenum'], label = i)\n",
    "    plt.ylabel('Density'); plt.title(str(df.name.values[0]));\n",
    "    plt.show()\n",
    "\n",
    "    # -- Output stats: Mean, median and standard deviation of the values\n",
    "    stats = (df.groupby('itemid')\n",
    "               .agg({'hadm_id': 'nunique',\n",
    "                     'valuenum': ['mean', 'median', 'std']})\n",
    "               .reset_index())\n",
    "    stats.columns = ['itemid', 'patients', 'mean', 'median', 'std']\n",
    "    print(stats)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, ids, sigma):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function takes a dataframe of chart observations and removes outliers. It visualises the statistical\n",
    "    distributions before and after to show the effect of removing the outliers.\n",
    "    \n",
    "    The arguments required for the function are:\n",
    "    1) df: the df containing the chart & lab data\n",
    "    2) ids: which column in the df contains the identifier that should be used. It should be different to itemid,\n",
    "       which enables comparison between all itemids assiciated with the new id\n",
    "    3) sigma: how many standard deviations should be used when identifying outliers.\n",
    "    \n",
    "    The input dataframe must be at the chart observation level, and have the following columns:\n",
    "    1) subject_id\n",
    "    2) hadm_id\n",
    "    3) charttime\n",
    "    4) itemid\n",
    "    5) valuenum\n",
    "    6) new_id - the new id that can link multiple itemids (passed in as 'ids')\n",
    "    7) name - the description of the new_id\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    new_ids = df[ids].unique().tolist()\n",
    "    print(\"Total new IDs: \", len(new_ids))\n",
    "\n",
    "    # Output df\n",
    "    cols = ['subject_id', 'hadm_id', 'charttime', 'itemid', 'valuenum', 'new_id', 'name']\n",
    "    new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    for i in new_ids:\n",
    "        \n",
    "        # Find just a single new id and its associated stats. Merge the stats onto the df\n",
    "        temp_df = df[df['new_id']==i]\n",
    "        \n",
    "        print()\n",
    "        print(\"=========\")\n",
    "        print(str(temp_df.name.values[0]))\n",
    "        print(\"=========\")\n",
    "        print()\n",
    "        print('Before removing outliers:')\n",
    "        \n",
    "        stats = compare_itemids(temp_df)\n",
    "        stats = stats[['itemid', 'mean', 'std']]\n",
    "        temp_df = pd.merge(temp_df, stats, how='left', left_on='itemid', right_on='itemid')\n",
    "\n",
    "        # Find outliers (based on sigma * std dev), setting these to np.nan and then looking at the new distributions\n",
    "        temp_df['lower'] = temp_df['mean'] - (sigma * temp_df['std'])\n",
    "        temp_df['upper'] = temp_df['mean'] + (sigma * temp_df['std'])\n",
    "        temp_df['valuenum'] = np.where((temp_df['valuenum'] > temp_df['lower'])\n",
    "                                       & (temp_df['valuenum'] < temp_df['upper']),\n",
    "                                      temp_df['valuenum'], np.nan)\n",
    "        \n",
    "        # Re-compare the item_ids to see if removing outliers has improved the distribution match\n",
    "        # Then remove the outliers and add the remaining observations to the output df\n",
    "        print()\n",
    "        print('After removing outliers:')\n",
    "        compare_itemids(temp_df)\n",
    "        temp_df.dropna(inplace=True)\n",
    "        new_df = new_df.append(temp_df[cols])\n",
    "\n",
    "    # Reset index\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # QA\n",
    "    print()\n",
    "    print(\"QA STATS:\")\n",
    "    print(\"Original DF length: \", len(df))\n",
    "    print(\"Original unique admissions: \", df.hadm_id.nunique())\n",
    "    print(\"New DF length: \", len(new_df))\n",
    "    print(\"New unique admissions: \", new_df.hadm_id.nunique())\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_outliers(df=df, ids='new_id', sigma=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- MANUAL CLEANING\n",
    "\n",
    "# Find equivalent min and max values from Carevue\n",
    "\n",
    "# Heart_rate\n",
    "hr_low = df[df['itemid']==211].valuenum.min()\n",
    "hr_high = df[df['itemid']==211].valuenum.max()\n",
    "\n",
    "# Respiratory_rate\n",
    "rr_low = df[df['itemid']==618].valuenum.min()\n",
    "rr_high = df[df['itemid']==618].valuenum.max()\n",
    "\n",
    "# Phosphorus\n",
    "p_low = df[(df['itemid']==827) | (df['itemid']==1534)].valuenum.min()\n",
    "p_high = df[(df['itemid']==827) | (df['itemid']==1534)].valuenum.max()\n",
    "\n",
    "# Oxygen saturation\n",
    "os_low = df[df['itemid']==834].valuenum.min()\n",
    "os_high = df[df['itemid']==834].valuenum.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HR (Metavision)\n",
    "def manual_range_change(df, new_id, low, high):\n",
    "    df['valuenum'] = np.where((df['valuenum']>high) & (df['new_id']==new_id), np.nan, df['valuenum'])\n",
    "    df['valuenum'] = np.where((df['valuenum']<low) & (df['new_id']==new_id), np.nan, df['valuenum'])\n",
    "    compare_itemids(df[df['new_id']==new_id])\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = manual_range_change(df, 9999018, hr_low, hr_high)\n",
    "df = manual_range_change(df, 9999025, rr_low, rr_high)\n",
    "df = manual_range_change(df, 9999022, p_low, p_high)\n",
    "df = manual_range_change(df, 9999040, os_low, os_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first reading per admission\n",
    "first_reading = (df.sort_values(by=['subject_id', 'hadm_id', 'new_id', 'charttime'])\n",
    "                   .groupby(['subject_id', 'hadm_id', 'new_id'])\n",
    "                   .first()\n",
    "                   .reset_index())\n",
    "\n",
    "# Pivot\n",
    "first_reading=(pd.pivot_table(first_reading, values='valuenum', index=['subject_id', 'hadm_id'], columns='name')\n",
    "                 .reset_index())\n",
    "print(\"Total df length: \", len(first_reading))\n",
    "\n",
    "to_s3(df=first_reading, bucket='mimic-jamesi', filename='first_reading.csv')\n",
    "first_reading.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
