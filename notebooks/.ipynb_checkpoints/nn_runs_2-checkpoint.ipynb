{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import statistics\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some display options\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/James/anaconda3/envs/mimic/bin/python'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check virtual environment: should be: '/Users/James/anaconda3/envs/mimic/bin/python'\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "src_folder = os.path.join(project_root, 'src')\n",
    "\n",
    "src_preparation_folder = os.path.join(src_folder, 'preparation')\n",
    "src_processing_folder = os.path.join(src_folder, 'processing')\n",
    "src_modeling_folder = os.path.join(src_folder, 'modeling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "# Import src functions\n",
    "sys.path.insert(0, src_preparation_folder)\n",
    "from import_data import get_table\n",
    "from import_data import get_data_simple\n",
    "from import_data import get_patient_admissions_diagnoses\n",
    "from import_data import get_admission_data\n",
    "from import_data import get_chartevents\n",
    "from import_data import get_labevents\n",
    "from extract_codes import find_ndc_codes\n",
    "\n",
    "sys.path.insert(0, src_processing_folder)\n",
    "from stats import plot_KDE\n",
    "from stats import plot_perc_bar_chart\n",
    "from stats import compare_groups\n",
    "from stats import graph_comparisons\n",
    "from patient_selection import select_test_groups\n",
    "from clean import replace_itemid_with_label\n",
    "from clean import find_populated_cols\n",
    "\n",
    "sys.path.insert(0, src_modeling_folder)\n",
    "from models import train_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train = pd.read_csv(os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'data', 'acute_respiratory_failure_train.csv')),index_col=0)\n",
    "test = pd.read_csv(os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'data', 'acute_respiratory_failure_test.csv')),index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cleaning(train, test):\n",
    "\n",
    "    # Shuffle\n",
    "    train = train.sample(frac=1).reset_index(drop=True)\n",
    "    test = test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Split features and labels\n",
    "    X_train = train.drop(columns=['subject_id', 'hadm_id', 'target'])\n",
    "    y_train = np.array(train.target.tolist())\n",
    "    \n",
    "    X_test = test.drop(columns=['subject_id', 'hadm_id', 'target'])\n",
    "    y_test = np.array(test.target.tolist())\n",
    "\n",
    "    # Impute missing values\n",
    "    imputer = Imputer(strategy = 'median')\n",
    "    imputer.fit(X_train)\n",
    "    X_train = imputer.transform(X_train)\n",
    "    X_test = imputer.transform(X_test)\n",
    "\n",
    "    # Scale each feature to 0-1\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1)) \n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22732, 42)\n",
      "(5684, 42)\n",
      "(22732,)\n",
      "(5684,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda3/envs/mimic/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = final_cleaning(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "hidden_layers = [0, 1, 2, 3, 4, 5]\n",
    "#weight_constraint = [1, 2, 3, 4, 5]\n",
    "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#learn_rate = [0.001, 0.01]\n",
    "#momentum = [0.0, 0.2]\n",
    "#neurons = [1]\n",
    "#hidden_layers = [0]\n",
    "weight_constraint = [1]\n",
    "dropout_rate = [0.0]\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate,\n",
    "                  momentum=momentum,\n",
    "                  neurons=neurons,\n",
    "                  hidden_layers=hidden_layers,\n",
    "                  weight_constraint=weight_constraint,\n",
    "                  dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(neurons, weight_constraint, dropout_rate, hidden_layers, learn_rate, momentum):\n",
    "    # Cleanup\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Initialize the constructor\n",
    "    model = Sequential()\n",
    "    # Add an input layer\n",
    "    model.add(Dense(output_dim=neurons,\n",
    "                    activation='relu',\n",
    "                    init = 'uniform',\n",
    "                    input_dim=X_train.shape[1],\n",
    "                    kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for i in range(hidden_layers):\n",
    "        # Add one hidden layer\n",
    "        model.add(Dense(output_dim=neurons,\n",
    "                        activation='relu',\n",
    "                        init = 'uniform',\n",
    "                        kernel_constraint=maxnorm(weight_constraint)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Add an output layer \n",
    "    model.add(Dense(output_dim=1, init = 'uniform', activation='sigmoid'))\n",
    "    \n",
    "    #compile model\n",
    "    optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "# create random search grid\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs=1, cv=5, n_iter=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x1a3b9b9470>,\n",
       "          fit_params=None, iid='warn', n_iter=5, n_jobs=1,\n",
       "          param_distributions={'learn_rate': [0.001, 0.01, 0.1, 0.2, 0.3], 'momentum': [0.0, 0.2, 0.4, 0.6, 0.8, 0.9], 'neurons': [1, 5, 10, 15, 20, 25, 30], 'hidden_layers': [0, 1, 2, 3, 4, 5], 'weight_constraint': [1], 'dropout_rate': [0.0]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mimic]",
   "language": "python",
   "name": "conda-env-mimic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
